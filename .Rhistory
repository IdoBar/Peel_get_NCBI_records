taxa <- read_csv("input_data/records-2023-03-16.csv") %>% select(`Species Name`) %>%
mutate(genebank.search=glue('{searchdef} "{`Species Name`}"[Organism]'))
# taxa <- taxa[,2]
# create test dataset
test_set <- head(taxa, n = 50)
test_search_string <- paste0(searchdef, " (",
paste(glue('"{test_set$`Species Name`}"[Organism]'), collapse = " OR "), ")")
# try to retrieve all search results for all species at once!!
res <- entrez_search(db="nuccore",
term=test_search_string,
use_history = TRUE)
View(res)
get_NCBI_info <- function(res_id){
# res_id = 1441206483
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', id = res_id, rettype = c("fasta"))
# res_features <- entrez_fetch(db="nuccore", id=res_id, rettype="text")
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',id = res_id) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi'), simplify = FALSE)
# res_summary[[1]]
summary_table <- res_summary %>% map_dfr(.f = as_tibble)
# summary_table$extra[2]
# create a table with the transcript info
sequence_info <- tibble(id = res_id, accession=res_summary$extra,
title= res_summary$title, taxid = res_summary$taxid,
organism = res_summary$organism, gi = res_summary$gi,
fasta = res_fasta, feature = res_summary$subtype)
return(sequence_info)
}
# use furrr and progressr to process these in parallel, see https://furrr.futureverse.org/articles/progress.html
plan(multisession, workers = min(4, availableCores())) # adjust cores based on your computer (but too many will cause the server to reject the requests)
# run in parallel (with progress bar)
# rerun command if it fails (see https://purrr.tidyverse.org/reference/insistently.html)
rate <- rate_delay(0.3, max_times = 10) # introduce a delay between queries to not to overload the server
insistent_retrieve_info <- insistently(get_NCBI_info, rate = rate, quiet = FALSE)
#NCBI recommends that users post no more than three URL requests per second and limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays.
results_data <- test_set$`Species Name` %>%
imap_dfr(.f = ~{
index = .y
species_name = .x
query <- glue('{searchdef} "{species_name}"[Organism]')
res <- entrez_search(db="nuccore",
term=query,
retmax=5000)
LogMsg(glue("Processing search query for '{species_name}' ({index}/{nrow(test_input)}), please wait..."))
with_progress({
p <- progressor(steps = length(res$ids))
results_table <- res$ids %>%
future_map_dfr(.f = ~{
id_res <- insistent_retrieve_info(.x)
p()
return(id_res)
})
})
# LogMsg(glue("Finished processing search query for '{species_name}', waiting 1 seconds before the next one"))
Sys.sleep(0.5) # introduce a delay between queries to not to overload the server
results_table %>% mutate(species_name = species_name, query = query)
})
View(test_set)
#NCBI recommends that users post no more than three URL requests per second and limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays.
results_data <- test_set$`Species Name` %>%
imap_dfr(.f = ~{
index = .y
species_name = .x
query <- glue('{searchdef} "{species_name}"[Organism]')
res <- entrez_search(db="nuccore",
term=query,
retmax=5000)
LogMsg(glue("Processing search query for '{species_name}' ({index}/{nrow(test_set)}), please wait..."))
with_progress({
p <- progressor(steps = length(res$ids))
results_table <- res$ids %>%
future_map_dfr(.f = ~{
id_res <- insistent_retrieve_info(.x)
p()
return(id_res)
})
})
# LogMsg(glue("Finished processing search query for '{species_name}', waiting 1 seconds before the next one"))
Sys.sleep(0.5) # introduce a delay between queries to not to overload the server
results_table %>% mutate(species_name = species_name, query = query)
})
# Cleaning Reference Databases - 60+ species --------------------------------------------
# Cinthia Pietromonaco & Ido Bar
# Test 1 - with species exported from ala within mbrc
# Rules: 1. complete sequence 2. herbarium voucher  3.ITS2  4. sequence length between 200-2500 5.Viridiplantae
#install.packages("pak") # best package ever to install other packages, see https://pak.r-lib.org/
library(pak)
#load packages
pacman::p_load(tidyverse, rentrez, janitor, glue, furrr, progressr, seqinr, taxonomizr)
#load packages
pacman::p_load(tidyverse, rentrez, janitor, glue, furrr, progressr, seqinr, taxonomizr)
# load custom functions that Ido wrote
devtools::source_gist("7f63547158ecdbacf31b54a58af0d1cc", filename = "Util.R")
# set NCBI API key - this allows yo to send more queries to Entrez without timing out - see more details at https://support.nlm.nih.gov/knowledgebase/article/KA-05318/en-us and https://support.nlm.nih.gov/knowledgebase/article/KA-05317/en-us
set_entrez_key("33ec35e1973e4928beafd47bee86492be109")
# Step 1: Search & Download Species List ----------------------------------
#string 1 for ITS2 marker
searchdef <- '((internal transcribed spacer[All Fields]) OR (ITS[All Fields])) AND (plants[filter]) AND (200[SLEN] : 2500[SLEN]) AND (flowering plants[porgn]) AND'
species_name <- "SPECIES_NAME"
#upload list
#make sure spelling is ok and if there are synonyms for certain species.
taxa <- read_csv("input_data/records-2023-03-16.csv") %>% select(`Species Name`) %>%
mutate(genebank.search=glue('{searchdef} "{`Species Name`}"[Organism]'))
# taxa <- taxa[,2]
# create test dataset
test_set <- head(taxa, n = 50)
test_search_string <- paste0(searchdef, " (",
paste(glue('"{test_set$`Species Name`}"[Organism]'), collapse = " OR "), ")")
#chunk searches query strings into groups of 50 species
species_chunks <- split(taxa$`Species Name`, ceiling(seq_along(taxa$`Species Name`)/50))
chunked_search_strings <- lapply(species_chunks, function(chunk) {
paste0(searchdef, " (",
paste(glue('"{taxa$`Species Name`}"[Organism]'), collapse = " OR "), ")")
})
chunked_search_strings <- enframe(chunked_search_strings)
# Step 2: Download FASTA files from GenBank ---------------------------------------
#entrez_db_summary('nucleotide')	 #checking its the right database
#entrez_db_searchable("nucleotide") #checking what search terms we can use
get_res_then_NCBI_info_from_web_history_ <- function(web_history_obj, rec_start, res){
res_history <- entrez_search(db="nuccore",
term=res,
use_history = TRUE)
return(data.frame(query = query, search_result = res$Count))
# rec_start = 1
# chunk_size=50
# web_history_obj = res$web_history
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', rettype = c("fasta")  , # id = res_id,
web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
# convert into a vector
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
stringi::stri_remove_empty_na() %>% paste0(">", .)
# res_features <- entrez_fetch(db="nuccore",  rettype="text", # id=res_id,
#                              web_history = web_history_obj ,
#                              retmax=chunk_size, retstart=rec_start)
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi', 'uid'), simplify = FALSE)
# res_summary[[1]]
summary_table <- res_summary %>%
map_dfr(.f = ~as_tibble(.x) %>% mutate(across(everything(), .fns = as.character)))
# summary_table$extra[2]
# create a table with the transcript info
sequence_info <- summary_table %>% rename(feature = subtype) %>%
mutate(fasta = fasta_vector, taxid = as.integer(taxid)) # also process accession out of the extra column
# sequence_info$organism
return(sequence_info)
}
# use furrr and progressr to process these in parallel, see https://furrr.futureverse.org/articles/progress.html
plan(multisession, workers = min(4, availableCores())) # adjust cores based on your computer (but too many will cause the server to reject the requests)
# run in parallel (with progress bar)
# rerun command if it fails (see https://purrr.tidyverse.org/reference/insistently.html)
rate <- rate_delay(0.3, max_times = 10) # introduce a delay between queries to not to overload the server
# Step 2: Download FASTA files from GenBank ---------------------------------------
#entrez_db_summary('nucleotide')	 #checking its the right database
#entrez_db_searchable("nucleotide") #checking what search terms we can use
get_res_then_NCBI_info_from_web_history <- function(web_history_obj, rec_start, res){
res_history <- entrez_search(db="nuccore",
term=res,
use_history = TRUE)
return(data.frame(query = query, search_result = res$Count))
# rec_start = 1
# chunk_size=50
# web_history_obj = res$web_history
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', rettype = c("fasta")  , # id = res_id,
web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
# convert into a vector
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
stringi::stri_remove_empty_na() %>% paste0(">", .)
# res_features <- entrez_fetch(db="nuccore",  rettype="text", # id=res_id,
#                              web_history = web_history_obj ,
#                              retmax=chunk_size, retstart=rec_start)
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi', 'uid'), simplify = FALSE)
# res_summary[[1]]
summary_table <- res_summary %>%
map_dfr(.f = ~as_tibble(.x) %>% mutate(across(everything(), .fns = as.character)))
# summary_table$extra[2]
# create a table with the transcript info
sequence_info <- summary_table %>% rename(feature = subtype) %>%
mutate(fasta = fasta_vector, taxid = as.integer(taxid)) # also process accession out of the extra column
# sequence_info$organism
return(sequence_info)
}
insistent_retrieve_info <- insistently(get_res_then_NCBI_info_from_web_history, rate = rate, quiet = FALSE)
#NCBI recommends that users post no more than three URL requests per second and limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays.
results_data <- chunked_search_strings$value %>%
imap_dfr(.f = ~{
index = .y
species_name = .x
query <- glue('{searchdef} "{species_name}"[Organism]')
res <- entrez_search(db="nuccore",
term=query,
retmax=5000)
LogMsg(glue("Processing search query for '{species_name}' ({index}/{nrow(test_input)}), please wait..."))
with_progress({
p <- progressor(steps = length(res$ids))
results_table <- res$ids %>%
future_map_dfr(.f = ~{
id_res <- insistent_retrieve_info(.x)
p()
return(id_res)
})
})
# LogMsg(glue("Finished processing search query for '{species_name}', waiting 1 seconds before the next one"))
Sys.sleep(0.5) # introduce a delay between queries to not to overload the server
results_table %>% mutate(species_name = species_name, query = query)
})
#NCBI recommends that users post no more than three URL requests per second and limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays.
results_data <- chunked_search_strings$value[1] %>%
imap_dfr(.f = ~{
index = .y
species_name = .x
query <- glue('{searchdef} "{species_name}"[Organism]')
res <- entrez_search(db="nuccore",
term=query,
retmax=5000)
LogMsg(glue("Processing search query for '{species_name}' ({index}/{nrow(test_input)}), please wait..."))
with_progress({
p <- progressor(steps = length(res$ids))
results_table <- res$ids %>%
future_map_dfr(.f = ~{
id_res <- insistent_retrieve_info(.x)
p()
return(id_res)
})
})
# LogMsg(glue("Finished processing search query for '{species_name}', waiting 1 seconds before the next one"))
Sys.sleep(0.5) # introduce a delay between queries to not to overload the server
results_table %>% mutate(species_name = species_name, query = query)
})
# Step 2: Download FASTA files from GenBank ---------------------------------------
#entrez_db_summary('nucleotide')	 #checking its the right database
#entrez_db_searchable("nucleotide") #checking what search terms we can use
get_res_then_NCBI_info_from_web_history <- function(web_history_obj, rec_start, res, chunk_size=100){
res_history <- entrez_search(db="nuccore",
term=res,
use_history = TRUE)
return(data.frame(query = query, search_result = res$Count))
# rec_start = 1
# chunk_size=50
# web_history_obj = res$web_history
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', rettype = c("fasta")  , # id = res_id,
web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
# convert into a vector
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
stringi::stri_remove_empty_na() %>% paste0(">", .)
# res_features <- entrez_fetch(db="nuccore",  rettype="text", # id=res_id,
#                              web_history = web_history_obj ,
#                              retmax=chunk_size, retstart=rec_start)
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi', 'uid'), simplify = FALSE)
# res_summary[[1]]
summary_table <- res_summary %>%
map_dfr(.f = ~as_tibble(.x) %>% mutate(across(everything(), .fns = as.character)))
# summary_table$extra[2]
# create a table with the transcript info
sequence_info <- summary_table %>% rename(feature = subtype) %>%
mutate(fasta = fasta_vector, taxid = as.integer(taxid)) # also process accession out of the extra column
# sequence_info$organism
return(sequence_info)
}
# process all ids from all species
# test_input <- head(taxa, 50)
chunk_size=100
#NCBI recommends that users post no more than three URL requests per second and limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays.
results_data <- chunked_search_strings$value%>%
imap_dfr(.f = ~{
index = .y
species_name = .x
query <- glue('{searchdef} "{species_name}"[Organism]')
res <- entrez_search(db="nuccore",
term=query,
retmax=5000)
LogMsg(glue("Processing search query for '{species_name}' ({index}/{nrow(test_input)}), please wait..."))
with_progress({
p <- progressor(steps = length(res$ids))
results_table <- res$ids %>%
future_map_dfr(.f = ~{
id_res <- insistent_retrieve_info(.x)
p()
return(id_res)
})
})
# LogMsg(glue("Finished processing search query for '{species_name}', waiting 1 seconds before the next one"))
Sys.sleep(0.5) # introduce a delay between queries to not to overload the server
results_table %>% mutate(species_name = species_name, query = query)
})
#NCBI recommends that users post no more than three URL requests per second and limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays.
results_data <- test_search_string %>%
imap_dfr(.f = ~{
index = .y
species_name = .x
query <- glue('{searchdef} "{species_name}"[Organism]')
res <- entrez_search(db="nuccore",
term=query,
retmax=5000)
LogMsg(glue("Processing search query for '{species_name}' ({index}/{nrow(test_input)}), please wait..."))
with_progress({
p <- progressor(steps = length(res$ids))
results_table <- res$ids %>%
future_map_dfr(.f = ~{
id_res <- insistent_retrieve_info(.x)
p()
return(id_res)
})
})
# LogMsg(glue("Finished processing search query for '{species_name}', waiting 1 seconds before the next one"))
Sys.sleep(0.5) # introduce a delay between queries to not to overload the server
results_table %>% mutate(species_name = species_name, query = query)
})
test_search_string
#NCBI recommends that users post no more than three URL requests per second and limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays.
results_data <- test_search_string[1] %>%
imap_dfr(.f = ~{
index = .y
species_name = .x
query <- glue('{searchdef} "{species_name}"[Organism]')
res <- entrez_search(db="nuccore",
term=query,
retmax=5000)
LogMsg(glue("Processing search query for '{species_name}' ({index}/{nrow(test_input)}), please wait..."))
with_progress({
p <- progressor(steps = length(res$ids))
results_table <- res$ids %>%
future_map_dfr(.f = ~{
id_res <- insistent_retrieve_info(.x)
p()
return(id_res)
})
})
# LogMsg(glue("Finished processing search query for '{species_name}', waiting 1 seconds before the next one"))
Sys.sleep(0.5) # introduce a delay between queries to not to overload the server
results_table %>% mutate(species_name = species_name, query = query)
})
#NCBI recommends that users post no more than three URL requests per second and limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays.
results_data <- test_search_string[1] %>%
imap_dfr(.f = ~{
index = .y
species_name = .x
query <- glue('{searchdef} "{species_name}"[Organism]')
res <- entrez_search(db="nuccore",
term=query,
retmax=5000)
LogMsg(glue("Processing search query for '{species_name}' ({index}/{nrow(test_search_string)}), please wait..."))
with_progress({
p <- progressor(steps = length(res$ids))
results_table <- res$ids %>%
future_map_dfr(.f = ~{
id_res <- insistent_retrieve_info(.x)
p()
return(id_res)
})
})
# LogMsg(glue("Finished processing search query for '{species_name}', waiting 1 seconds before the next one"))
Sys.sleep(0.5) # introduce a delay between queries to not to overload the server
results_table %>% mutate(species_name = species_name, query = query)
})
View(results_data)
# Step 2: Download FASTA files from GenBank ---------------------------------------
#entrez_db_summary('nucleotide')	 #checking its the right database
#entrez_db_searchable("nucleotide") #checking what search terms we can use
get_res_then_NCBI_info_from_web_history <- function(web_history_obj, rec_start, res, chunk_size=100){
res_history <- entrez_search(db="nuccore",
term=res,
use_history = TRUE)
return(data.frame(query = query, search_result = res$Count))
# rec_start = 1
# chunk_size=50
# web_history_obj = res$web_history
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', rettype = c("fasta")  , # id = res_id,
web_history = res_history,
retmax=chunk_size, retstart=rec_start)
# convert into a vector
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
stringi::stri_remove_empty_na() %>% paste0(">", .)
# res_features <- entrez_fetch(db="nuccore",  rettype="text", # id=res_id,
#                              web_history = web_history_obj ,
#                              retmax=chunk_size, retstart=rec_start)
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi', 'uid'), simplify = FALSE)
# res_summary[[1]]
summary_table <- res_summary %>%
map_dfr(.f = ~as_tibble(.x) %>% mutate(across(everything(), .fns = as.character)))
# summary_table$extra[2]
# create a table with the transcript info
sequence_info <- summary_table %>% rename(feature = subtype) %>%
mutate(fasta = fasta_vector, taxid = as.integer(taxid)) # also process accession out of the extra column
# sequence_info$organism
return(sequence_info)
}
insistent_retrieve_info <- insistently(get_res_then_NCBI_info_from_web_history, rate = rate, quiet = FALSE)
# process all ids from all species
# test_input <- head(taxa, 50)
chunk_size=100
#NCBI recommends that users post no more than three URL requests per second and limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays.
results_data <- chunked_search_strings$value %>%
imap_dfr(.f = ~{
index = .y
species_name = .x
query <- glue('{searchdef} "{species_name}"[Organism]')
res <- entrez_search(db="nuccore",
term=query,
retmax=5000)
LogMsg(glue("Processing search query for '{species_name}' ({index}/{nrow(test_input)}), please wait..."))
with_progress({
p <- progressor(steps = length(res$ids))
results_table <- res$ids %>%
future_map_dfr(.f = ~{
id_res <- insistent_retrieve_info(.x)
p()
return(id_res)
})
})
# LogMsg(glue("Finished processing search query for '{species_name}', waiting 1 seconds before the next one"))
Sys.sleep(0.5) # introduce a delay between queries to not to overload the server
results_table %>% mutate(species_name = species_name, query = query)
})
View(chunked_search_strings)
chunked_search_strings <- apply(species_chunks, function(chunk) {
paste0(searchdef, " (",
paste(glue('"{taxa$`Species Name`}"[Organism]'), collapse = " OR "), ")")
})
#chunk searches query strings into groups of 50 species
species_chunks <- split(taxa$`Species Name`, ceiling(seq_along(taxa$`Species Name`)/50))
chunked_search_strings <- lapply(species_chunks, function(chunk) {
paste0(searchdef, " (",
paste(glue('"{taxa$`Species Name`}"[Organism]'), collapse = " OR "), ")")
})
chunked_search_strings <- unlist(chunked_search_strings)
View(chunked_search_strings)
chunked_search_strings <- data.frame(chunked_search_strings)
View(chunked_search_strings)
# Step 2: Download FASTA files from GenBank ---------------------------------------
#entrez_db_summary('nucleotide')	 #checking its the right database
#entrez_db_searchable("nucleotide") #checking what search terms we can use
get_res_then_NCBI_info_from_web_history <- function(web_history_obj, rec_start, res, chunk_size=100){
res_history <- entrez_search(db="nuccore",
term=res,
use_history = TRUE)
return(data.frame(query = query, search_result = res$Count))
# rec_start = 1
# chunk_size=50
# web_history_obj = res$web_history
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', rettype = c("fasta")  , # id = res_id,
web_history = res_history,
retmax=chunk_size, retstart=rec_start)
# convert into a vector
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
stringi::stri_remove_empty_na() %>% paste0(">", .)
# res_features <- entrez_fetch(db="nuccore",  rettype="text", # id=res_id,
#                              web_history = web_history_obj ,
#                              retmax=chunk_size, retstart=rec_start)
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi', 'uid'), simplify = FALSE)
# res_summary[[1]]
summary_table <- res_summary %>%
map_dfr(.f = ~as_tibble(.x) %>% mutate(across(everything(), .fns = as.character)))
# summary_table$extra[2]
# create a table with the transcript info
sequence_info <- summary_table %>% rename(feature = subtype) %>%
mutate(fasta = fasta_vector, taxid = as.integer(taxid)) # also process accession out of the extra column
# sequence_info$organism
return(sequence_info)
}
#NCBI recommends that users post no more than three URL requests per second and limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays.
results_data <- chunked_search_strings$chunked_search_strings %>%
imap_dfr(.f = ~{
index = .y
species_name = .x
query <- glue('{searchdef} "{species_name}"[Organism]')
res <- entrez_search(db="nuccore",
term=query,
retmax=5000)
LogMsg(glue("Processing search query for '{species_name}' ({index}/{nrow(test_input)}), please wait..."))
with_progress({
p <- progressor(steps = length(res$ids))
results_table <- res$ids %>%
future_map_dfr(.f = ~{
id_res <- insistent_retrieve_info(.x)
p()
return(id_res)
})
})
# LogMsg(glue("Finished processing search query for '{species_name}', waiting 1 seconds before the next one"))
Sys.sleep(0.5) # introduce a delay between queries to not to overload the server
results_table %>% mutate(species_name = species_name, query = query)
})
