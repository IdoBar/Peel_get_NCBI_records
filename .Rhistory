library(tidyverse) # this includes all of the packages above
# Step 1: Search & Download Species List ----------------------------------
#upload list
#make sure spelling is ok and if there are synonyms for certain species.
taxa <- read_csv("records-2022-12-01.csv")
# Step 1: Search & Download Species List ----------------------------------
#upload list
#make sure spelling is ok and if there are synonyms for certain species.
taxa <- read_csv("records-2022-12-01.csv") %>% select(`Species Name`)
taxa
# lists greater than 60 species -------------------------------------------
add.taxa <- taxa %>%
mutate(spp_string = str_replace(species.str,"SPECIES_NAME", `Species Name`))
add.taxa
library(glue)
#string for species (do you have the special double quotes in purpose or is t because it was copied-pasted from a web page or a word document?)
species.str <- '“SPECIES_NAME”[Organism] OR '
# lists greater than 60 species -------------------------------------------
add.taxa <- taxa %>%
mutate(spp_string = str_replace(species.str,"SPECIES_NAME", `Species Name`))
add.taxa
# Step 1: Search & Download Species List ----------------------------------
#upload list
#make sure spelling is ok and if there are synonyms for certain species.
taxa <- read_csv("records-2022-12-01.csv") %>% select(`Species Name`) %>%
mutate(spp_string=glue('"{`Species Name`}"[Organism] OR'))
taxa
#create empty vector to store output
genbank.search <- c()
for(i in seq(1, nrow(add.taxa), 30)) {
tmp <- paste(add.taxa$spp_string[i:(i+29)], collapse = " ")
# Remove trailing NA for groups with less than 60 species
tmp <- gsub("(\\sNA){2,}$", "", tmp)
# remove trailing " OR "
tmp <- gsub(" OR $", "", tmp)
tmp <- paste(tmp, sep ="", collapse = "")
tmp <- paste('(', tmp, sep ="", collapse = "")
tmp <- paste(tmp, ')', sep ="", collapse = "")
tmp <- paste(searchdef, tmp)
genbank.search <- c(genbank.search, tmp)
}
#string 1 for ITS2 marker
searchdef <- '((internal transcribed spacer[All Fields]) OR (ITS[All Fields])) AND (plants[filter]) AND (200[SLEN] : 2500[SLEN]) AND (flowering plants[porgn]) AND'
for(i in seq(1, nrow(add.taxa), 30)) {
tmp <- paste(add.taxa$spp_string[i:(i+29)], collapse = " ")
# Remove trailing NA for groups with less than 60 species
tmp <- gsub("(\\sNA){2,}$", "", tmp)
# remove trailing " OR "
tmp <- gsub(" OR $", "", tmp)
tmp <- paste(tmp, sep ="", collapse = "")
tmp <- paste('(', tmp, sep ="", collapse = "")
tmp <- paste(tmp, ')', sep ="", collapse = "")
tmp <- paste(searchdef, tmp)
genbank.search <- c(genbank.search, tmp)
}
genbank.search
?insistently
# Step 2: Download FASTA files from GenBank ---------------------------------------
library(rentrez)
??detect
#load packages
pacman::p_load(tidyverse, rentrez, janitor, glue, furrr, progressr, seqinr, taxonomizr)
?plan
parallel::detectCores()
availableCores()
# set NCBI API key - this allows yo to send more queries to Entrez without timing out - see more details at https://support.nlm.nih.gov/knowledgebase/article/KA-05318/en-us and https://support.nlm.nih.gov/knowledgebase/article/KA-05317/en-us
set_entrez_key("33ec35e1973e4928beafd47bee86492be109")
library(pak)
# install packages for parallel processing, see https://furrr.futureverse.org/articles/progress.html
pkg_install(c("furrr", "progressr"))
# install utility packages that make life easy
# pacman provides the easiest way to load packages in one line, see https://trinker.github.io/pacman/vignettes/Introduction_to_pacman.html
pkg_install(c("pacman", "janitor"))
# install bioinformatics-related packages
pkg_install(c("rentrez", "seqinr", "myTAI", "taxize", "taxonomizr"))
# load custom functions that Ido wrote
devtools::source_gist("7f63547158ecdbacf31b54a58af0d1cc", filename = "Util.R")
#load packages
pacman::p_load(tidyverse, rentrez, janitor, glue, furrr, progressr, seqinr, taxonomizr)
# set NCBI API key - this allows yo to send more queries to Entrez without timing out - see more details at https://support.nlm.nih.gov/knowledgebase/article/KA-05318/en-us and https://support.nlm.nih.gov/knowledgebase/article/KA-05317/en-us
set_entrez_key("33ec35e1973e4928beafd47bee86492be109")
# Step 1: Search & Download Species List ----------------------------------
#string 1 for ITS2 marker
searchdef <- '((internal transcribed spacer[All Fields]) OR (ITS[All Fields])) AND (plants[filter]) AND (200[SLEN] : 2500[SLEN]) AND (flowering plants[porgn]) AND'
species_name <- “SPECIES_NAME”
species_name <- "SPECIES_NAME"
#upload list
#make sure spelling is ok and if there are synonyms for certain species.
taxa <- read_csv("records-2022-12-01.csv") %>% select(`Species Name`) %>%
mutate(genebank.search=glue('{searchdef} "{`Species Name`}"[Organism]'))
#upload list
#make sure spelling is ok and if there are synonyms for certain species.
taxa <- read_csv("input_data/records-2023-03-16.csv") %>% select(`Species Name`) %>%
mutate(genebank.search=glue('{searchdef} "{`Species Name`}"[Organism]'))
res_id = 1441206483
rec_start = 1
chunk_size=5
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', id = res_id, rettype = c("fasta","text")  ,
# web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', id = res_id, rettype = c("fasta")  ,
# web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
chunk_size=1
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', id = res_id, rettype = c("fasta")  ,
# web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
res_fasta <- entrez_fetch(db='nuccore', id = res_id, rettype = c("fasta"))
res_fasta
# test on one species name
res <- entrez_search(db="nuccore",
term=taxa$genebank.search[1],
retmax=5000, use_history = TRUE)
#string for species (do you have the special double quotes in purpose or is t because it was copied-pasted from a web page or a word document?)
# species.str <- '"SPECIES_NAME"[Organism] OR '
test_set <- head(taxa, n = 50)
test_set
test_search_string <- paste(glue('"{taxa$`Species Name`}"[Organism]'), collapse = " OR ")
test_search_string
test_search_string <- paste(searchdef,
paste(glue('"{taxa$`Species Name`}"[Organism]'), collapse = " OR "))
?entrez_search
# try to retrieve all search results for all species at once!!
res <- entrez_search(db="nuccore",
term=test_search_string,
use_history = TRUE)
test_search_string <- paste(searchdef,
paste(glue('"{test_set$`Species Name`}"[Organism]'), collapse = " OR "))
# try to retrieve all search results for all species at once!!
res <- entrez_search(db="nuccore",
term=test_search_string,
use_history = TRUE)
# taxa <- taxa[,2]
# create test dataset
test_set <- head(taxa, n = 500)
test_search_string <- paste(searchdef,
paste(glue('"{test_set$`Species Name`}"[Organism]'), collapse = " OR "))
# try to retrieve all search results for all species at once!!
res <- entrez_search(db="nuccore",
term=test_search_string,
use_history = TRUE)
# taxa <- taxa[,2]
# create test dataset
test_set <- head(taxa, n = 100)
test_search_string <- paste(searchdef,
paste(glue('"{test_set$`Species Name`}"[Organism]'), collapse = " OR "))
# try to retrieve all search results for all species at once!!
res <- entrez_search(db="nuccore",
term=test_search_string,
use_history = TRUE)
# taxa <- taxa[,2]
# create test dataset
test_set <- head(taxa, n = 50)
test_search_string <- paste(searchdef,
paste(glue('"{test_set$`Species Name`}"[Organism]'), collapse = " OR "))
# try to retrieve all search results for all species at once!!
res <- entrez_search(db="nuccore",
term=test_search_string,
use_history = TRUE)
res$web_history$WebEnv
res
res$web_history
res$web_history$QueryKey
chunk_size=50
web_history_obj = res$web_history
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', rettype = c("fasta")  , # id = res_id,
web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
res_fasta
res_features <- entrez_fetch(db="nuccore",  rettype="text", # id=res_id,
web_history = web_history_obj ,
retmax=chunk_size, retstart=rec_start)
res_features
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype'))
res_summary
res_summary$title
str(res_summary)
res_summary[[1]]
?extract_from_esummary
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype'), simplify = FALSE)
res_summary[[1]]
res_summary[[1]] %>% enframe()
test <- res_summary[[1]] %>% enframe()
unlist(test$value)
res_summary[[1]] %>% as_tibble()
summary_table <- res_summary %>% map_dfr(.f = as_tibble)
summary_table
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'species'), simplify = FALSE)
summary_table <- res_summary %>% map_dfr(.f = as_tibble)
res_summary[[1]]
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism'), simplify = FALSE)
res_summary[[1]]
summary_table <- res_summary %>% map_dfr(.f = as_tibble)
summary_table
?entrez_fetch
test <- str_split(res_fasta, pattern = ">")
test
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'accession'), simplify = FALSE)
res_summary[[1]]
entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start)
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi'), simplify = FALSE)
res_summary[[1]]
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi', 'uid'), simplify = FALSE)
res_summary
res_summary[[1]]
test[1]
test[[1]]
summary_table
summary_table <- res_summary %>% map_dfr(.f = as_tibble)
summary_table$extra
test[1][1]
test[[1]][1]
test <- str_split(res_fasta, pattern = ">") %>% unlist()
test[1]
test
str(test)
test[2]
summary_table$extra[1]
summary_table$extra[2]
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
.[2:length(.)]
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
.[2:length(.)]
summary_table %>% rename(accession=extra, feature = subtype)
# # extract title from summary
# res_title <- extract_from_esummary(res_summary, c("title", "extra"))
# # extract accession from summary
# res_acc <- extract_from_esummary(res_summary, "extra")
# create a table with the transcript info
sequence_info <- summary_table %>% rename(feature = subtype) %>%
mutate(fasta = paste0(">", fasta_vector)) # also process accession out of the extra column
sequence_info
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist()
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
stringi::stri_remove_empty_na()
get_NCBI_info_from_web_history <- function(web_history_obj, rec_start, chunk_size=100){
# rec_start = 1
# chunk_size=50
# web_history_obj = res$web_history
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', rettype = c("fasta")  , # id = res_id,
web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
res_features <- entrez_fetch(db="nuccore",  rettype="text", # id=res_id,
web_history = web_history_obj ,
retmax=chunk_size, retstart=rec_start)
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi', 'uid'), simplify = FALSE)
# res_summary[[1]]
summary_table <- res_summary %>% map_dfr(.f = as_tibble)
# summary_table$extra[2]
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
stringi::stri_remove_empty_na()
# create a table with the transcript info
sequence_info <- summary_table %>% rename(feature = subtype) %>%
mutate(fasta = paste0(">", fasta_vector)) # also process accession out of the extra column
return(sequence_info)
}
# # combine results
test_res <- get_NCBI_info_from_web_history(web_history_obj = res$web_history,
rec_start = 1)
# rec_start = 1
# chunk_size=50
# web_history_obj = res$web_history
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', rettype = c("fasta")  , # id = res_id,
web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
res_features <- entrez_fetch(db="nuccore",  rettype="text", # id=res_id,
web_history = web_history_obj ,
retmax=chunk_size, retstart=rec_start)
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi', 'uid'), simplify = FALSE)
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start)
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj)
res_features
res_fasta
# test on one species name
res <- entrez_search(db="nuccore",
term=taxa$genebank.search[1],
retmax=5000, use_history = TRUE)
# test on one species name
res <- entrez_search(db="nuccore",
term=taxa$genebank.search[1],
retmax=5000, use_history = TRUE)
# # combine results
test_res <- get_NCBI_info_from_web_history(web_history_obj = res$web_history,
rec_start = 1)
rlang::last_error()
?across
get_NCBI_info_from_web_history <- function(web_history_obj, rec_start, chunk_size=100){
# rec_start = 1
# chunk_size=50
# web_history_obj = res$web_history
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', rettype = c("fasta")  , # id = res_id,
web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
res_features <- entrez_fetch(db="nuccore",  rettype="text", # id=res_id,
web_history = web_history_obj ,
retmax=chunk_size, retstart=rec_start)
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi', 'uid'), simplify = FALSE)
# res_summary[[1]]
summary_table <- res_summary %>%
map_dfr(.f = ~as_tibble(.x) %>% mutate(across(everything(), .fns = as.character)))
# summary_table$extra[2]
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
stringi::stri_remove_empty_na()
# create a table with the transcript info
sequence_info <- summary_table %>% rename(feature = subtype) %>%
mutate(fasta = paste0(">", fasta_vector)) # also process accession out of the extra column
return(sequence_info)
}
# # combine results
test_res <- get_NCBI_info_from_web_history(web_history_obj = res$web_history,
rec_start = 1)
web_history_obj = res$web_history
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi', 'uid'), simplify = FALSE)
res_summary
web_history_obj
res_features
# rec_start = 1
# chunk_size=50
# web_history_obj = res$web_history
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', rettype = c("fasta")  , # id = res_id,
web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
res_features <- entrez_fetch(db="nuccore",  rettype="text", # id=res_id,
web_history = web_history_obj ,
retmax=chunk_size, retstart=rec_start)
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start)
res_summary
# test on one species name
res <- entrez_search(db="nuccore",
term=test_search_string,
retmax=5000, use_history = TRUE)
# rec_start = 1
# chunk_size=50
# web_history_obj = res$web_history
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', rettype = c("fasta")  , # id = res_id,
web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
stringi::stri_remove_empty_na()
test_search_string
test_search_string <- paste0(searchdef, " (",
paste(glue('"{test_set$`Species Name`}"[Organism]'), collapse = " OR "), ")")
test_search_string
# try to retrieve all search results for all species at once!!
res <- entrez_search(db="nuccore",
term=test_search_string,
use_history = TRUE)
res$web_history
web_history_obj = res$web_history
# rec_start = 1
# chunk_size=50
# web_history_obj = res$web_history
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', rettype = c("fasta")  , # id = res_id,
web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
# convert into a vector
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
stringi::stri_remove_empty_na() %>% paste0(">", .)
?str_split
# convert into a vector
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
stringi::stri_remove_empty_na() %>% paste0(">", .)
fasta_vector[1]
# res_features <- entrez_fetch(db="nuccore",  rettype="text", # id=res_id,
#                              web_history = web_history_obj ,
#                              retmax=chunk_size, retstart=rec_start)
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi', 'uid'), simplify = FALSE)
# res_summary[[1]]
summary_table <- res_summary %>%
map_dfr(.f = ~as_tibble(.x) %>% mutate(across(everything(), .fns = as.character)))
summary_table
# create a table with the transcript info
sequence_info <- summary_table %>% rename(feature = subtype) %>%
mutate(fasta = fasta_vector) # also process accession out of the extra column
sequence_info
# create a table with the transcript info
sequence_info <- summary_table %>% rename(feature = subtype) %>%
mutate(fasta = fasta_vector, taxid = as.integer(taxid)) # also process accession out of the extra column
sequence_info
sequence_info$organism
get_NCBI_info_from_web_history <- function(web_history_obj, rec_start, chunk_size=100){
# rec_start = 1
# chunk_size=50
# web_history_obj = res$web_history
# fetch this entry as FASTA
res_fasta <- entrez_fetch(db='nuccore', rettype = c("fasta")  , # id = res_id,
web_history = web_history_obj,
retmax=chunk_size, retstart=rec_start)
# convert into a vector
fasta_vector <- str_split(res_fasta, pattern = ">") %>% unlist() %>%
stringi::stri_remove_empty_na() %>% paste0(">", .)
# res_features <- entrez_fetch(db="nuccore",  rettype="text", # id=res_id,
#                              web_history = web_history_obj ,
#                              retmax=chunk_size, retstart=rec_start)
# fetch this entry's summary info
res_summary <- entrez_summary(db='nuccore',web_history = web_history_obj, #  id = res_id,
retmax=chunk_size, retstart=rec_start) %>%
extract_from_esummary(., c("title", "extra", "taxid", 'subtype', 'organism',
'gi', 'uid'), simplify = FALSE)
# res_summary[[1]]
summary_table <- res_summary %>%
map_dfr(.f = ~as_tibble(.x) %>% mutate(across(everything(), .fns = as.character)))
# summary_table$extra[2]
# create a table with the transcript info
sequence_info <- summary_table %>% rename(feature = subtype) %>%
mutate(fasta = fasta_vector, taxid = as.integer(taxid)) # also process accession out of the extra column
# sequence_info$organism
return(sequence_info)
}
# # combine results
test_res <- get_NCBI_info_from_web_history(web_history_obj = res$web_history,
rec_start = 1)
test_res
test_res %>% count(title)
test_res %>% dplyr::count(title)
# run in parallel (with progress bar)
# rerun command if it fails (see https://purrr.tidyverse.org/reference/insistently.html)
rate <- rate_delay(0.3, max_times = 10) # introduce a delay between queries to not to overload the server
insistent_retrieve_info <- insistently(get_NCBI_info_from_web_history, rate = rate, quiet = FALSE)
# use furrr and progressr to process these in parallel, see https://furrr.futureverse.org/articles/progress.html
plan(multisession, workers = min(4, availableCores())) # adjust cores based on your computer (but too many will cause the server to reject the requests)
test_res
res
str(res)
seq(1,res$count,50)
res$count
# process all ids from all species
# test_input <- head(taxa, 50)
chunk_size=100
seq(1,res$count,chunk_size)
# # process results
test_res <- seq(1,res$count,chunk_size) %>%
map_dfr(~get_NCBI_info_from_web_history(web_history_obj = res$web_history,
rec_start = .x, chunk_size = chunk_size))
